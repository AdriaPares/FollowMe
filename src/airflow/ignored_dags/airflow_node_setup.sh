#!/bin/sh

# Konrad's Instructions. My comments are in @@

#NOTE this was performed on a ubuntu 18.04 LTS OS, t2 micro, with autogenerated
#     public IP and in same VPC as all pipeline components to be automated

# install any missing python3 resources on actual machine
sudo apt-get install python3-setuptools
sudo easy_install3 pip

# get updates
sudo apt update

# run to check that you are at latest version of pip (19.1.1):
sudo pip install --upgrade pip

# "pip3 --version" and "pip --version" at this point should both tell you python 3.5 pip 19.1.1
# #NOTE from here on out pip points to pip3

# install, name and activate virtual environment for airflow to run on
pip install --user virtualenv
#virtualenv ~/venv

#set python 3.5 to be default for the virtual environment!
virtualenv -p /usr/bin/python3.5 ~/venv

source ~/venv/bin/activate

# start loading up virtual environment with everything we need for airflow to run
sudo apt update

sudo apt -y install python3-pip
sudo pip install --upgrade pip

#***POSTGRESS INSTALLATION + CONFIG***
#install postgresql
sudo apt-get -y install postgresql postgresql-contrib

#create database for airflow & configure...
sudo -u postgres psql

#NOTE: Following are Postgres commands to be entered after above
'''
CREATE ROLE ubuntu;
CREATE DATABASE airflow;
GRANT ALL PRIVILEGES on database airflow to ubuntu;
ALTER ROLE ubuntu SUPERUSER;
ALTER ROLE ubuntu CREATEDB;
ALTER ROLE ubuntu WITH LOGIN;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public to ubuntu;
'''
# (still in postgres..)NOTE we now connect to our 'airflow' database:
'''
\c airflow
'''
#RESPONSE: 'You are now connected to database "airflow" as user "postgres".'

'''
\conninfo
'''
#RESPONSE:'You are connected to database "airflow" as user "postgres" via socket in "/var/run/postgresql" at port "5432".'

'''
SHOW hba_file;
'''
#RESPONSE:
#               hba_file
#--------------------------------------
# /etc/postgresql/9.5/main/pg_hba.conf
#NOTE This shows us the location of the hba_file, we will need to edit in the next step!

'''
\q
'''

# jump into nano/vim and change the following..
sudo vi /etc/postgresql/9.5/main/pg_hba.conf

#NOTE: in the 'IPv4 local connections' section we make the following change:

@@ NOT SURE ABOUT THE TRUST THING...
'''
# IPv4 local connections:
#host    all             all             127.0.0.1/32            md5
host    all             all             0.0.0.0/0               trust
'''
#Then save and exit!
#In the same directory we now edit another file: postgresql.conf:
sudo vi /etc/postgresql/9.5/main/postgresql.conf

@@ SEEMS SUPER DANGEROUS, PROBABLY BETTER TO KEEP AS LOCALHOST

#NOTE: in the 'CONNECTIONS AND AUTHENTICATION' section we add the new line below:
'''
#listen_addresses = 'localhost'         # what IP address(es) to listen on;
listen_addresses = '*'  #(ADDED)
'''
#save, exit and restart postgresql..
sudo service postgresql restart

#***AIRFLOW INSTALLATION + CONFIG***

#set airflow environment variable
export AIRFLOW_HOME=~/airflow
#'echo $AIRFLOW_HOME' should return: '/home/ubuntu/airflow'

#Install Airflow Dependencies:
sudo apt-get -y install libmysqlclient-dev
sudo apt-get -y install libssl-dev
sudo apt-get -y install libkrb5-dev
sudo apt-get -y install libsasl2-dev

#Install Airflow along with all packages we will be using to run it parallel and
#across nodes:
pip install "apache-airflow[celery,postgres,rabbitmq]"

#initialize airflow database
@@ ONLY DO THIS ONCE
airflow initdb

#jump in and edit the airflow.cfg file..
sudo vi airflow/airflow.cfg

#within airflow.cfg we change the following lines:
'''
#executor = SequentialExecutor
executor = CeleryExecutor
'''

'''
#sql_alchemy_conn = sqlite:////home/ubuntu/airflow/airflow.db
sql_alchemy_conn = postgresql+psycopg2://ubuntu@localhost:5432/airflow
'''

'''
#load_examples = True
load_examples = False
'''

'''
#broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflow
broker_url = amqp://guest:guest@localhost:5672//
'''

@@ AIRFLOW COMPLAINS ABOUT THIS
'''
#result_backend = db+mysql://airflow:airflow@localhost:3306/airflow
result_backend = amqp://guest:guest@localhost:5672//
'''
#save and exit, then re-initialize airflow database:
airflow initdb

#create a DAGs folder:
mkdir -p /home/ubuntu/airflow/dags/

#***STARTING AIRFLOW***
#NOTE: '-D' parameter is so we start each service in a Daemon and can leave it
#      running in the background while we start other services.
#      Shutting down daemons is covered below..
airflow webserver -D
airflow scheduler -D
airflow worker -D

#***ACCESSING AIRFLOW ADMIN PAGE***
#NOTE: For every IP you want to access it from you will need to add an IAM rule!
#       Add "Custom TCP Rule  TCP 8080  104.247.55.102/32" for insight
#       Add "Custom TCP Rule  TCP 8080  _._._._/_" for home

#then go to: http://ec2-_-_-_-_.compute-1.amazonaws.com:8080/admin/ to view admin interface
#plug in your airflow instance's Public DNS (IPv4) from the ec2 dashboard description tab
#into above url to get to admin page!

#***STOPPING AIRFLOW***
#TODO: pipe PIDs into kill command
#get process id's & kill them:
cat $AIRFLOW_HOME/airflow-webserver.pid
sudo kill -9 _____ #NOTE ____ insert process ID!!
#sudo kill -9 < cat $AIRFLOW_HOME/airflow-webserver.pid

#TODO why dont we see the worker PID?
#sudo kill -9 < cat $AIRFLOW_HOME/airflow-worker.pid
cat $AIRFLOW_HOME/airflow-scheduler.pid
sudo kill -9 _____
#sudo kill -9 < cat $AIRFLOW_HOME/airflow-scheduler.pid

#check they were killed with the following (no output means killed successfully):
ps -ef | grep airflow-scheduler | grep -v grep
ps -ef | grep airflow-server | grep -v grep
#TODO possibly resolve warnings associated with 'airflow worker -D' command:
##UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
##{default_celery.py:90} WARNING - You have configured a result_backend of amqp://guest:guest@localhost:5672//, it is highly recommended to use an alternative result_backend (i.e. a database).


#GUIDES USED:
'''
https://vujade.co/install-apache-airflow-ubuntu-18-04/
https://medium.com/@taufiq_ibrahim/apache-airflow-installation-on-ubuntu-ddc087482c14
https://adataguru.net/setup-airflow/
https://airflow.apache.org/installation.html
https://www.astronomer.io/guides/airflow-executors-explained/
'''

# You also need to setup a public key between the Ariflow instance and the Spark Master
