## Workflow of simulations when starting cluster from zero

1) Read player_accounts, player_original_dates, player_count (tables in Cassandra, created on spawn, kept constant)
[[For now, assume they are in random_accounts.json]]

1.5) Simulate 12h of live data with semi-bounded random walk (3*3600*12 = 130k files per player)

2) Generate daily data until today-2. Use a random variation (capped at 1% of actual count) as your end point.
Load to Cassandra

[At this point, Day contains all but 2 days, rest is empty]

3) Pick up from last generated day: this is now our starting count. Generate 48 hours + all but two hours until
 the current hour of hourly data (we can use the actual count already). Load to Cassandra (this can be less than 48h)

[At this point, Day contains all but 2 rows, Hour contains all but 2 rows, rest is empty]

4) Pick up from last generated hour: this is now our starting count. Generate 120 minutes of minute data.
Plus some extra minutes to let Spark jobs finish and not have a delay. This can be avoided if we start the pipeline
two hours before demo. Load to Cassandra

[At this point, Day contains all but 2 rows, Hour contains all but 2 rows, minutes is up to date, live is empty]

5) Fire Spark jobs minute_to_hour, then hour_to_day. [All but live are up to date]

6) Fire the whole pipeline