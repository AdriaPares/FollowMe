# SPARK CLuster

# For now, we are using a m5.large 80GiB for master, and 2 c5.xlarge 100 GiB as slaves for now,
# scale up slaves until c5.4xlarge if
# you need more power, at that point scale out.

# Good tutorial: https://blog.insightdatascience.com/simply-install-spark-cluster-mode-341843a52b88


sudo apt-get update

#install java 8

sudo apt install openjdk-8-jre -y

# install scala

sudo apt install scala -y

# install sbt. Check https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Linux.html for up to date links, for Ubuntu 16.04

echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823
sudo apt-get update
sudo apt-get install sbt

# install Spark 2.4.3. Check official Spark page for Spark version and hadoop requirements.

wget http://apache.mirrors.tds.net/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz -P ~/Downloads
sudo tar zxvf ~/Downloads/spark-2.4.3-bin-hadoop2.7.tgz -C /usr/local
sudo mv /usr/local/spark-2.4.3-bin-hadoop2.7 /usr/local/spark

# Not sure if this is required...
sudo chown -R ubuntu /usr/local/spark

# Edit ~/.profile, add the following, run source .profile. Check that the homes are set with env:

# Spark
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin

# Note: when installing pyspark, we will change SPARK_HOME...


# Configure Spark

cp /usr/local/spark/conf/spark-env.sh.template /usr/local/spark/conf/spark-env.sh


# A word on SPARK_WORKER_CORES:
# Oversubscription: It is the idea of exaggerating the number of resources that is available for use
# so cluster manager is in the illusion of having more resources. This forces CPU to work on a task and not sit
# idle during any clock cycles. You can set this by setting the SPARK_WORKER_CORES flag in spark-env.sh to a value
# higher than the actual number of cores.
# For example, in our setup, our workers have 4 cores, and we have 2 workers. Hence, we can set
# SPARK_WOKER_CORES = 8

# Check this for more info: https://stackoverflow.com/questions/26645293/spark-configuration-memory-instance-cores
# However, we still want overworking
# Open spark-env.sh and add:

# export JAVA_HOME=/usr/lib/jvm/openjdk-8-jre-amd64
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export SPARK_PUBLIC_DNS="<MASTER-public-dns>" (like ec2-x-xx-xxxx-xx.compute1.amazonaws.com
export SPARK_WORKER_CORES=[[[NUMBER CALCULATED HERE]]]

# Configure Worker nodes on master. Go to master node and run:

touch $SPARK_HOME/conf/slaves

# Add each worker to the file. Alternatively, just open $SPARK_HOME/conf/slaves and copy the public DNS (one per line)

# public ips here because it allows SparUI to handle them better
echo <slave-public-dns> | cat >> $SPARK_HOME/conf/slaves

# Connect master to workers. THESE COMMANDS ARE FOR MASTER ONLY

sudo apt install openssh-server openssh-client
cd ~/.ssh
ssh-keygen -t rsa -P ""

Generating public/private rsa key pair.
Enter file in which to save the key: id_rsa
Your identification has been saved in id_rsa.
Your public key has been saved in id_rsa.pub.


# Manually copy to slaves.
# On master:
cat ~/.ssh/id_rsa.pub
# On slaves:
vi ~/.ssh/authorized_keys
# paste the key

# Try to connect master to workers
ssh ubuntu@ec2.x--x-x-x-x-x

# Start the server:

sh /usr/local/spark/sbin/start-all.sh

# Check everything is working by going to the master_public_ip:8080 (requires port 8080 open).
# If you see the SparkUI with your workers up, it's done.
# Move to pyspark installation